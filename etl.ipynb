{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.functions import udf, col, to_timestamp, monotonically_increasing_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/song/song_data/*/*/*/*.json\n",
      "data/log/*.json\n",
      "data/output_data/\n"
     ]
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "\n",
    "config.read_file(open('dl.cfg'))\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]= config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "#AWS_ACCESS_KEY_ID=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "#AWS_SECRET_ACCESS_KEY= config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "#INPUT_DATA_SD = config['AWS']['INPUT_DATA_SD']\n",
    "#INPUT_DATA_LD = config['AWS']['INPUT_DATA_LD']\n",
    "#OUTPUT_DATA = config['AWS']['OUTPUT_DATA']\n",
    "SONG_DATA_LOCAL=config['LOCAL']['INPUT_DATA_SD_LOCAL']\n",
    "LOG_DATA_LOCAL=config['LOCAL']['INPUT_DATA_LD_LOCAL']\n",
    "OUTPUT_DATA_LOCAL=config['LOCAL']['OUTPUT_DATA_LOCAL']\n",
    "\n",
    "#print(AWS_ACCESS_KEY_ID)\n",
    "#print(AWS_SECRET_ACCESS_KEY)\n",
    "##print(INPUT_DATA)\n",
    "#print(OUTPUT_DATA)\n",
    "print(SONG_DATA_LOCAL)\n",
    "print(LOG_DATA_LOCAL)\n",
    "print(OUTPUT_DATA_LOCAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                     .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "                     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read local song_data\n",
    "song_data_path = SONG_DATA_LOCAL\n",
    "#output_data_path = OUTPUT_DATA_LOCAL\n",
    "output_data_path = OUTPUT_DATA_LOCAL\n",
    "log_data_path = LOG_DATA_LOCAL\n",
    "# Use this instead if you want to read song_data from S3.\n",
    "#song_data_path = INPUT_DATA_SD\n",
    "#usersParquetPath = os.path.join(output_data_path, \"users\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/output_data/'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#df_sd.printSchema()\n",
    "#df_sd.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_song_data(spark, input_data, output_data_path):\n",
    "    \"\"\"\n",
    "    Description: \n",
    "    This function read the JSON file of song data process the columns and create dimension tables\n",
    "    \n",
    "    Parameters:\n",
    "        spark: the spark object.\n",
    "        input_path: the path from where the data is read.\n",
    "        output_path: the destination path for parquete file\n",
    "            will be stored.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "     #===== Step-1: Load song_data from local or S3=====\n",
    "    start = datetime.now()\n",
    "    \n",
    "    print(\"Start reading song_data JSON files...\")\n",
    "\n",
    "    \n",
    "    \n",
    "    # get filepath to song data file\n",
    "    song_data = input_data\n",
    "\n",
    "    # read data file\n",
    "    df = spark.read.json(song_data)\n",
    "    print('Success: Read song_data DONE!! ...')\n",
    "    \n",
    "    stop = datetime.now()\n",
    "    total_st = stop - start\n",
    "    print(\"Total time reading song_data {}.\".format(total_st))\n",
    "    print(\"Show song_data schema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "   \n",
    "   \n",
    " #===== Step-2: read song_data columns from dataframe and write to parquet file=====\n",
    " \n",
    "    # extract columns to create songs table\n",
    "    start = datetime.now()\n",
    "    songs_table = df.select('song_id', 'title', 'artist_id',\n",
    "                            'year', 'duration').dropDuplicates()\n",
    "\n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table.write.parquet(f'{output_data_path}/songs_table',\n",
    "                              mode='overwrite',\n",
    "                              partitionBy=['year', 'artist_id'])\n",
    "    print('Success: songs_table to parquet DONE')\n",
    "    print('print schema ...')\n",
    "    songs_table.printSchema()\n",
    "    parquet_schema=spark.read.parquet(f'{output_data_path}/songs_table')\n",
    "    print('print parquet schema ...')\n",
    "    parquet_schema.show(5)\n",
    "    stop = datetime.now()\n",
    "    total_st = stop - start\n",
    "    print(\"Total time writing songs_table {}.\".format(total_st))\n",
    "    \n",
    "    start = datetime.now()\n",
    " #===== Step-3: read artist_column columns from dataframe and write to parquet file ====\n",
    "    # extract columns to create artists table\n",
    "    artists_table = df.select('artist_id', 'artist_name',\n",
    "                              'artist_location', 'artist_latitude',\n",
    "                              'artist_longitude').dropDuplicates()\n",
    "\n",
    "    # write artists table to parquet files\n",
    "    artists_table.write.parquet(f'{output_data_path}/artists_table',\n",
    "                                mode='overwrite')\n",
    "    print('Success: artists_table to parquet DONE !!')\n",
    "    print('print schema ...')\n",
    "    \n",
    "    artists_table.printSchema()\n",
    "    parquet_schema=spark.read.parquet(f'{output_data_path}/artists_table')\n",
    "    print('print parquet schema ...')\n",
    "    parquet_schema.show(5)\n",
    "    stop = datetime.now()\n",
    "    total_at = stop - start\n",
    "    print(\"Time writing artists_table {}.\".format(total_at))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start reading song_data JSON files...\n",
      "Success: Read song_data DONE!! ...\n",
      "Total time reading song_data 0:00:01.162468.\n",
      "Show song_data schema:\n",
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n",
      "Success: songs_table to parquet DONE\n",
      "print schema ...\n",
      "root\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      "\n",
      "print parquet schema ...\n",
      "+------------------+--------------------+---------+----+------------------+\n",
      "|           song_id|               title| duration|year|         artist_id|\n",
      "+------------------+--------------------+---------+----+------------------+\n",
      "|SOAOIBZ12AB01815BE|I Hold Your Hand ...| 43.36281|2000|ARPBNLO1187FB3D52F|\n",
      "|SONYPOM12A8C13B2D7|I Think My Wife I...|186.48771|2005|ARDNS031187B9924F0|\n",
      "|SODREIN12A58A7F2E5|A Whiter Shade Of...|326.00771|   0|ARLTWXK1187FB5A3F8|\n",
      "|SOYMRWW12A6D4FAB14|The Moon And I (O...| 267.7024|   0|ARKFYS91187B98E58F|\n",
      "|SOWQTQZ12A58A7B63E|Streets On Fire (...|279.97995|   0|ARPFHN61187FB575F6|\n",
      "+------------------+--------------------+---------+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total time writing songs_table 0:00:05.336687.\n",
      "Success: artists_table to parquet DONE !!\n",
      "print schema ...\n",
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      "\n",
      "print parquet schema ...\n",
      "+------------------+--------------------+--------------------+---------------+----------------+\n",
      "|         artist_id|         artist_name|     artist_location|artist_latitude|artist_longitude|\n",
      "+------------------+--------------------+--------------------+---------------+----------------+\n",
      "|ARNF6401187FB57032|   Sophie B. Hawkins|New York, NY [Man...|       40.79086|       -73.96644|\n",
      "|AROUOZZ1187B9ABE51|         Willie Bobo|New York, NY [Spa...|       40.79195|       -73.94512|\n",
      "|AREBBGV1187FB523D2|Mike Jones (Featu...|         Houston, TX|           null|            null|\n",
      "|ARD842G1187B997376|          Blue Rodeo|Toronto, Ontario,...|       43.64856|       -79.38533|\n",
      "|ARDR4AC1187FB371A1|Montserrat Caball...|                    |           null|            null|\n",
      "+------------------+--------------------+--------------------+---------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Time writing artists_table 0:00:03.930798.\n"
     ]
    }
   ],
   "source": [
    "process_song_data(spark, song_data_path, output_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pyspark.sql.functions as F\n",
    "def process_log_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Description: \n",
    "    This function is used to read the log and song data, transform the time \n",
    "    column and create fact and dimension table.\n",
    "    \n",
    "    Parameters:\n",
    "        spark: the spark object.\n",
    "        input_path: Path to read the data.\n",
    "        output_path: The output path for storing parquet file\n",
    "           \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # get filepath to log data file\n",
    "    log_data = input_data\n",
    "#===== Step-1: Load song_data from local or S3=====\n",
    "    start = datetime.now()\n",
    "    #start_sdl = datetime.now()\n",
    "    print(\"Start reading log_data JSON files...\")\n",
    "    \n",
    "    \n",
    "    # read log data file\n",
    "    df = spark.read.json(log_data)\n",
    "    print('Success: Read log_data DONE!! ...')\n",
    "\n",
    "    stop = datetime.now()\n",
    "    total_st = stop - start\n",
    "    print(\"Total time reading log_data {}.\".format(total_st))\n",
    "    print(\"Show log_data schema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    " #===== Step-2: read log_data selected columns from dataframe and write to parquet file=====    \n",
    "    start = datetime.now()\n",
    "    # filter by nextSong\n",
    "    df = df.filter(df['page'] == 'NextSong')\n",
    "\n",
    "    # extract columns for users table\n",
    "    user_table = df.select('userId', 'firstName', 'lastName',\n",
    "                           'gender', 'level').dropDuplicates()\n",
    "#\"songs_table.write.mode('overwrite').partitionBy('year','artist_id').parquet(output_data + 'songs_table')\n",
    "    # write users table to parquet files\n",
    "    user_table.write.parquet( f'{output_data}/user_table', mode='overwrite')\n",
    "    print('Success: Wrote user_table to parquet DONE')\n",
    "\n",
    "    stop = datetime.now()\n",
    "    total_st = stop - start\n",
    "    print(\"Total time reading user_table {}.\".format(total_st))\n",
    "    print(\"Show user_table schema:\")\n",
    "    user_table.printSchema()\n",
    "    print(\"Show user_table parquete schema:\")\n",
    "    parquet_schema=spark.read.parquet(f'{output_data}/user_table')\n",
    "    parquet_schema.show(5)\n",
    "    \n",
    "    \n",
    "#===== Step-3: creating time_table tranforming columns from dataframe and write to parquet file=====    \n",
    "    start = datetime.now()\n",
    "    \n",
    "    # create timestamp column from original timestamp column\n",
    "    df = df.withColumn('start_time', F.from_unixtime(F.col('ts')/1000))\n",
    "    print('Success: Convert ts to timestamp DONE')\n",
    "\n",
    "    # create datetime column from original timestamp column\n",
    "    time_table = df.select('ts', 'start_time') \\\n",
    "                   .withColumn('year', F.year('start_time')) \\\n",
    "                   .withColumn('month', F.month('start_time')) \\\n",
    "                   .withColumn('week', F.weekofyear('start_time')) \\\n",
    "                   .withColumn('weekday', F.dayofweek('start_time')) \\\n",
    "                   .withColumn('day', F.dayofyear('start_time')) \\\n",
    "                   .withColumn('hour', F.hour('start_time')).dropDuplicates()\n",
    "    print('Success: Extract DateTime Columns DONE')\n",
    "\n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table.write.parquet(f'{output_data}/time_table',\n",
    "                             mode='overwrite',\n",
    "                             partitionBy=['year', 'month'])\n",
    "    print('Success: Wrote time_table to parquet Done')\n",
    "    \n",
    "    stop = datetime.now()\n",
    "    total_st = stop - start\n",
    "    print(\"Sucsess: Total time reading time_table {}.\".format(total_st))\n",
    "    print(\"Show time_table schema:\")\n",
    "    time_table.printSchema()\n",
    "    print(\"Show time_table parquete schema:\")\n",
    "    ddf=spark.read.parquet(f'{output_data}/time_table')\n",
    "    ddf.show(5)\n",
    "    \n",
    "#===== Step-4: reading song data=====    \n",
    "    start = datetime.now()\n",
    "    \n",
    "    # read in song data to use for songplays table\n",
    "    song_data = song_data_path\n",
    "    song_dataset = spark.read.json(song_data)\n",
    "    print('Success: Read song_dataset DONE')\n",
    "    \n",
    "    stop = datetime.now()\n",
    "    total_st = stop - start\n",
    "    print(\"Sucsess: Total time reading song data {}.\".format(total_st))\n",
    "    print(\"Show song_table schema:\")\n",
    "    song_dataset.printSchema()\n",
    "    \n",
    "    \n",
    "#===== Step-4: creating  songplays_table=====    \n",
    "    start = datetime.now()\n",
    "    \n",
    "    # join & extract cols from song and log datasets to create songplays table\n",
    "    song_dataset.createOrReplaceTempView('song_dataset')\n",
    "    time_table.createOrReplaceTempView('time_table')\n",
    "    df.createOrReplaceTempView('log_dataset')\n",
    "\n",
    "    songplays_table = spark.sql(\"\"\"SELECT \n",
    "                                       l.ts as ts,\n",
    "                                       t.year as year,\n",
    "                                       t.month as month,\n",
    "                                       l.userId as user_id,\n",
    "                                       l.level as level,\n",
    "                                       s.song_id as song_id,\n",
    "                                       s.artist_id as artist_id,\n",
    "                                       l.sessionId as session_id,\n",
    "                                       s.artist_location as artist_location,\n",
    "                                       l.userAgent as user_agent\n",
    "                                   FROM song_dataset s\n",
    "                                   JOIN log_dataset l\n",
    "                                       ON s.artist_name = l.artist\n",
    "                                       \n",
    "                                   JOIN time_table t\n",
    "                                       ON t.ts = l.ts\n",
    "                                   \"\"\").dropDuplicates()\n",
    "    print('Success: SQL DONE')\n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table.write.parquet(f'{output_data}/songplays_table',\n",
    "                                  mode='overwrite',\n",
    "                                  partitionBy=['year', 'month'])\n",
    "    print('Success: Wrote songplays_table to parquet')\n",
    "    stop = datetime.now()\n",
    "    total_st = stop - start\n",
    "    print(\"Sucsess: Total time creating songplays_table {}.\".format(total_st))\n",
    "    print(\"Show songplays_table schema:\")\n",
    "    songplays_table.printSchema()\n",
    "    print(\"Show songplays_table parquete schema:\")\n",
    "    parquet_schema=spark.read.parquet(f'{output_data}/songplays_table')\n",
    "    totals=parquet_schema.count()\n",
    "    print(\"Show songplays_table total lines:\",totals )\n",
    "    parquet_schema.show(10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start reading log_data JSON files...\n",
      "Success: Read log_data DONE!! ...\n",
      "Total time reading log_data 0:00:00.370410.\n",
      "Show log_data schema:\n",
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n",
      "Success: Wrote user_table to parquet DONE\n",
      "Total time reading user_table 0:00:04.802462.\n",
      "Show user_table schema:\n",
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      "\n",
      "Show user_table parquete schema:\n",
      "+------+---------+---------+------+-----+\n",
      "|userId|firstName| lastName|gender|level|\n",
      "+------+---------+---------+------+-----+\n",
      "|    88| Mohammad|Rodriguez|     M| paid|\n",
      "|    88| Mohammad|Rodriguez|     M| free|\n",
      "|    11|Christian|   Porter|     F| free|\n",
      "|    53|  Celeste| Williams|     F| free|\n",
      "|    69| Anabelle|  Simpson|     F| free|\n",
      "+------+---------+---------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Success: Convert ts to timestamp DONE\n",
      "Success: Extract DateTime Columns DONE\n",
      "Success: Wrote time_table to parquet Done\n",
      "Sucsess: Total time reading time_table 0:00:07.753482.\n",
      "Show time_table schema:\n",
      "root\n",
      " |-- ts: long (nullable = true)\n",
      " |-- start_time: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      "\n",
      "Show time_table parquete schema:\n",
      "+-------------+-------------------+----+-------+---+----+----+-----+\n",
      "|           ts|         start_time|week|weekday|day|hour|year|month|\n",
      "+-------------+-------------------+----+-------+---+----+----+-----+\n",
      "|1542306165796|2018-11-15 18:22:45|  46|      5|319|  18|2018|   11|\n",
      "|1542317096796|2018-11-15 21:24:56|  46|      5|319|  21|2018|   11|\n",
      "|1542321342796|2018-11-15 22:35:42|  46|      5|319|  22|2018|   11|\n",
      "|1542764840796|2018-11-21 01:47:20|  47|      4|325|   1|2018|   11|\n",
      "|1542791112796|2018-11-21 09:05:12|  47|      4|325|   9|2018|   11|\n",
      "+-------------+-------------------+----+-------+---+----+----+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Success: Read song_dataset DONE\n",
      "Sucsess: Total time reading song data 0:00:01.127036.\n",
      "Show song_table schema:\n",
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n",
      "Success: SQL DONE\n",
      "Success: Wrote songplays_table to parquet\n",
      "Sucsess: Total time creating songplays_table 0:00:06.490244.\n",
      "Show songplays_table schema:\n",
      "root\n",
      " |-- ts: long (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- session_id: long (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- user_agent: string (nullable = true)\n",
      "\n",
      "Show songplays_table parquete schema:\n",
      "Show songplays_table total lines: 21\n",
      "+-------------+-------+-----+------------------+------------------+----------+--------------------+--------------------+----+-----+\n",
      "|           ts|user_id|level|           song_id|         artist_id|session_id|     artist_location|          user_agent|year|month|\n",
      "+-------------+-------+-----+------------------+------------------+----------+--------------------+--------------------+----+-----+\n",
      "|1542837407796|     15| paid|SOZCTXZ12AB0182364|AR5KOSW1187FB35FF4|       818|           Dubai UAE|\"Mozilla/5.0 (X11...|2018|   11|\n",
      "|1543031301796|     88| paid|SONWXQJ12A8C134D94|ARNF6401187FB57032|       888|New York, NY [Man...|\"Mozilla/5.0 (Mac...|2018|   11|\n",
      "|1541967558796|     80| paid|SOWTBJW12AC468AC6E|ARQGYP71187FB44566|       435|         Mineola, AR|\"Mozilla/5.0 (Mac...|2018|   11|\n",
      "|1542660241796|     42| paid|SOFFKZS12AB017F194|ARBEBBY1187B9B43DB|       632|     Gainesville, FL|\"Mozilla/5.0 (Win...|2018|   11|\n",
      "|1542666522796|     25| paid|SOFFKZS12AB017F194|ARBEBBY1187B9B43DB|       594|     Gainesville, FL|\"Mozilla/5.0 (Win...|2018|   11|\n",
      "|1543255030796|     29| paid|SOQVMXR12A81C21483|ARKULSX1187FB45F84|       924|                Utah|\"Mozilla/5.0 (Mac...|2018|   11|\n",
      "|1543447377796|     24| paid|SOWQTQZ12A58A7B63E|ARPFHN61187FB575F6|       984|         Chicago, IL|\"Mozilla/5.0 (Win...|2018|   11|\n",
      "|1541520284796|      2| free|SOWQTQZ12A58A7B63E|ARPFHN61187FB575F6|       126|         Chicago, IL|\"Mozilla/5.0 (Win...|2018|   11|\n",
      "|1542226599796|    101| free|SORRZGD12A6310DBC3|ARVBRGZ1187FB4675A|       603|                    |\"Mozilla/5.0 (Win...|2018|   11|\n",
      "|1542313967796|     44| paid|SOBONFF12A6D4F84D8|ARIK43K1187B9AE54C|       619|   Beverly Hills, CA|Mozilla/5.0 (Maci...|2018|   11|\n",
      "+-------------+-------+-----+------------------+------------------+----------+--------------------+--------------------+----+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "total record count of songplay table DataFrame[count(1): bigint]\n"
     ]
    }
   ],
   "source": [
    "process_log_data(spark, log_data_path, output_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
